{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook1e3a0acef5",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft trl bitsandbytes wandb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T17:35:22.383631Z",
          "iopub.execute_input": "2025-05-05T17:35:22.384055Z",
          "iopub.status.idle": "2025-05-05T17:36:37.827901Z",
          "shell.execute_reply.started": "2025-05-05T17:35:22.384026Z",
          "shell.execute_reply": "2025-05-05T17:36:37.826944Z"
        },
        "id": "aB5QrYtG3qHd",
        "outputId": "13ebd2e8-dd2e-48fc-89bd-566fa7b3ad5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting trl\n  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.17.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "wandb.login(key=secret_value_0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T17:36:37.8292Z",
          "iopub.execute_input": "2025-05-05T17:36:37.82946Z",
          "iopub.status.idle": "2025-05-05T17:36:46.363548Z",
          "shell.execute_reply.started": "2025-05-05T17:36:37.829438Z",
          "shell.execute_reply": "2025-05-05T17:36:46.362687Z"
        },
        "id": "kSltJHzr3qHe",
        "outputId": "1c618d47-70ed-467b-de8c-fc2aa287200e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmudit-jain2303\u001b[0m (\u001b[33mmudit-jain2303-mait\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install all required packages for Colab\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import DPOTrainer,DPOConfig\n",
        "import bitsandbytes as bnb\n",
        "# from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "# Authentication tokens (defined in Google Colab secrets tab)\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")   # Make sure to add this to Colab secrets\n",
        "# wb_token = userdata.get('wandb')  # Optional: for tracking experiments\n",
        "# if wb_token:\n",
        "#     wandb.login(key=wb_token)\n",
        "\n",
        "# Model configuration\n",
        "base_model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # Updated to Qwen2.5-7B-Instruct\n",
        "new_model_name = \"Socratic-Qwen2.5-7B-v2\"  # Updated model name\n",
        "\n",
        "# Load your dataset\n",
        "dataset = load_dataset(\"mudit23/class7-socratic-dpo\", token=hf_token)['train']\n",
        "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
        "print(f\"Sample columns: {dataset.column_names}\")\n",
        "\n",
        "\n",
        "\n",
        "# Load tokenizer with Qwen2.5 specific settings\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=hf_token, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Function to format your dataset entries according to Qwen2.5's chat template\n",
        "def format_dpo_dataset(example):\n",
        "    \"\"\"\n",
        "    Format dataset for DPO training.\n",
        "    Handles:\n",
        "    - 'question': string\n",
        "    - 'chosen': list of {'role': ..., 'content': ...} with dialogue\n",
        "    - 'rejected': string (less preferred assistant answer)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Format the prompt using Qwen2.5 chat template\n",
        "    system_message = {\"role\": \"system\", \"content\": \"You are a helpful, harmless, and honest assistant.\"}\n",
        "    user_message = {\"role\": \"user\", \"content\": example['question'].strip()}\n",
        "    messages = [system_message, user_message]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # 2. Reconstruct the assistant's (teacher's) preferred answer\n",
        "    # In your dataset, 'teacher' represents the assistant, so we collect those turns only\n",
        "    chosen_turns = [msg['content'].strip() for msg in example['chosen'] if msg['role'] == 'teacher']\n",
        "    chosen_response = \"\\n\".join(chosen_turns)\n",
        "\n",
        "    # 3. Rejected is already a plain assistant reply\n",
        "    rejected_response = example['rejected'].strip()\n",
        "\n",
        "    # 4. Ensure EOS tokens\n",
        "    if not chosen_response.endswith(tokenizer.eos_token):\n",
        "        chosen_response += tokenizer.eos_token\n",
        "    if not rejected_response.endswith(tokenizer.eos_token):\n",
        "        rejected_response += tokenizer.eos_token\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen_response,\n",
        "        \"rejected\": rejected_response,\n",
        "    }\n",
        "\n",
        "# Print an example from raw dataset\n",
        "print(\"\\nRaw dataset example:\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Format the dataset\n",
        "original_columns = dataset.column_names\n",
        "formatted_dataset = dataset.map(\n",
        "    format_dpo_dataset,\n",
        "    remove_columns=original_columns\n",
        ")\n",
        "\n",
        "split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']\n",
        "print(len(train_dataset))\n",
        "print(len(eval_dataset))\n",
        "\n",
        "# Print example of formatted dataset\n",
        "print(\"\\nFormatted dataset example:\")\n",
        "print(formatted_dataset[0])\n",
        "\n",
        "# Setup quantization configuration - optimized for Qwen2.5\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the base model with quantization - Qwen2.5 specific settings\n",
        "print(\"\\nLoading Qwen2.5 model. This may take a few minutes...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=hf_token,\n",
        "    # attn_implementation=\"flash_attention_2\"  # Using Flash Attention 2 for Qwen2.5\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# LoRA configuration for Qwen2.5 - target modules specific to Qwen2.5 architecture\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Increased rank for better adaptation to Qwen2.5\n",
        "    lora_alpha=32,  # Adjusted alpha for Qwen2.5\n",
        "    lora_dropout=0.05,  # Reduced dropout for better performance with Qwen2.5\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target modules specific to Qwen2.5 architecture\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        \"w1\", \"w2\", \"w3\"  # Additional projection layers in Qwen2.5\n",
        "    ]\n",
        ")\n",
        "\n",
        "# # Training arguments - optimized for Qwen2.5 with ~700 examples\n",
        "training_args = DPOConfig(\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,# Smaller batch size for Qwen2.5 due to larger model\n",
        "    gradient_accumulation_steps=16,  # Increased for stable training with Qwen2.5\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=5e-6,  # Lower learning rate for Qwen2.5\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=200,  # Slightly more steps for Qwen2.5\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=1,  # Keep only the best 2 checkpoints\n",
        "    logging_steps=10,\n",
        "    output_dir=new_model_name,\n",
        "    optim=\"paged_adamw_8bit\",  # More memory efficient optimizer for Qwen2.5\n",
        "    warmup_steps=50,  # Increased warmup for Qwen2.5\n",
        "    bf16=False,  # Use BF16 for better training stability with Qwen2.5\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    report_to=\"wandb\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    model_init_kwargs=None,\n",
        "    max_prompt_length=768,  # Increased for Qwen2.5's context handling\n",
        "    max_length=1536,\n",
        "    beta=0.2,\n",
        ")\n",
        "\n",
        "# Create DPO trainer with Qwen2.5 specific settings\n",
        "print(\"\\nInitializing DPO trainer for Qwen2.5...\")\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nStarting DPO training for Qwen2.5...\")\n",
        "dpo_trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "print(\"\\nSaving model...\")\n",
        "dpo_trainer.model.save_pretrained(\"final_checkpoint\")\n",
        "tokenizer.save_pretrained(\"final_checkpoint\")\n",
        "\n",
        "# Clean up resources\n",
        "del dpo_trainer, model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload base model in FP16 (instead of NF4)\n",
        "print(\"\\nReloading Qwen2.5 base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "# Merge base model with the adapter\n",
        "print(\"\\nMerging with LoRA weights...\")\n",
        "model = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "print(\"\\nSaving merged model...\")\n",
        "model.save_pretrained(new_model_name)\n",
        "tokenizer.save_pretrained(new_model_name)\n",
        "\n",
        "# Push to HF Hub (if desired)\n",
        "if hf_token:\n",
        "    print(\"\\nPushing model to Hugging Face Hub...\")\n",
        "    model.push_to_hub(new_model_name, use_temp_dir=False, token=hf_token)\n",
        "    tokenizer.push_to_hub(new_model_name, use_temp_dir=False, token=hf_token)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T17:37:08.997721Z",
          "iopub.execute_input": "2025-05-05T17:37:08.998644Z",
          "iopub.status.idle": "2025-05-05T23:23:28.880649Z",
          "shell.execute_reply.started": "2025-05-05T17:37:08.998614Z",
          "shell.execute_reply": "2025-05-05T23:23:28.879406Z"
        },
        "id": "omeN4D7G3qHe",
        "outputId": "ae741ab7-a327-42c3-a7a9-51fa2b2909cd",
        "colab": {
          "referenced_widgets": [
            "6939fa6221da4df08f45bf560d8f2f3d",
            "413299a669234fc0943f04aedc341244",
            "f7458a9d66444c86b87929dd63aedbb0",
            "7c654c6dd2b74ce588c4996102fa6a0b",
            "d1db93c80f454475b241b10cded3aabd",
            "cdd4b7af577b481ebc8a7de10843fca8",
            "af43530fa6954508b246c952d3b6925c",
            "c695a6f4c5614e79aae2a6fd8e878fe3",
            "7d590d98f9884e78bfc5af6dfa13c522",
            "3fc42517b70f47d98494f1bf7ca3293a",
            "6f016a5904754bd8937a0ad4663db882",
            "f564cd5627e543e2bf69c67b433212c9",
            "78a42b03d3544c7899486135f67a02a5",
            "0925207fa7fa4c678a56583be7f61abd",
            "e14ef5c9735c454094ae34d923ae90ba",
            "38508a9583204a0fafbece0db500f34b",
            "4cf0197443884a30bace8f3529a2c3b4",
            "9a5c4beb16de43e88228f15114953d18",
            "48fdc0f9d7dc48eb8443cfe01bd56cd1",
            "b97407dca5514c779b47dc0b3e8b9dfe",
            "69899c2880764eecb10ee42ebe350951",
            "ba399e444ec54a6eaa19d36142055e4d",
            "ff470c082a3a4292be8daf9414d65edc",
            "553f23158ccc49e78d58324d394e9d03",
            "fc962617a67c40dd9644ef9f378e7e98",
            "edb96158125c4197bcfad4728d89a351",
            "21ecf5006dc24cbb9ad2e4b4ea21513b",
            "0cebd94db95840aabc83febad3c25cc3",
            "efba37c573614ec3ba51becfd0004fc2",
            "611bfc64219b40c9928736c3202ace84",
            "1fb20c5397ea451081c71cff1f978bda"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-05-05 17:37:22.363269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746466642.555809      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746466642.612869      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/463 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6939fa6221da4df08f45bf560d8f2f3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/776k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "413299a669234fc0943f04aedc341244"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/734 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7458a9d66444c86b87929dd63aedbb0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset loaded with 734 examples\nSample columns: ['question', 'rejected', 'chosen']\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c654c6dd2b74ce588c4996102fa6a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1db93c80f454475b241b10cded3aabd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdd4b7af577b481ebc8a7de10843fca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af43530fa6954508b246c952d3b6925c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRaw dataset example:\n{'question': 'What early studies were inspired by simple observations like paper planes?', 'rejected': 'According to NCERT Class 7 Science, the early studies inspired by simple observations like paper planes are the studies of aerodynamics and the work of Leonardo da Vinci.', 'chosen': [{'content': 'What do you think is the connection between paper planes and early studies in science?', 'role': 'teacher'}, {'content': \"Hmm, I'm not sure. Maybe it's because paper planes fly in the air?\", 'role': 'student'}, {'content': \"That's a good start! Flight is definitely related to paper planes. Can you think of someone who might be interested in studying how things fly?\", 'role': 'teacher'}, {'content': 'Like pilots or people who build airplanes?', 'role': 'student'}, {'content': 'Exactly! Now, can you think of someone who lived a long time ago, before airplanes were invented, but might still be interested in how things fly?', 'role': 'teacher'}, {'content': 'Maybe someone like... Leonardo da Vinci?', 'role': 'student'}, {'content': \"That's correct! Leonardo da Vinci made many drawings of flying machines, including birds in flight. His studies on bird flight and wing structure actually laid the foundation for modern aerodynamics. And it all started with simple observations, much like throwing a paper plane.\", 'role': 'teacher'}, {'content': \"So, to summarize, simple observations like paper planes can lead to early studies in science, just like how Leonardo da Vinci's observations on bird flight inspired his research on aerodynamics. Great job thinking it through! \", 'role': 'teacher'}]}\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/734 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c695a6f4c5614e79aae2a6fd8e878fe3"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "660\n74\n\nFormatted dataset example:\n{'rejected': 'According to NCERT Class 7 Science, the early studies inspired by simple observations like paper planes are the studies of aerodynamics and the work of Leonardo da Vinci.<|im_end|>', 'chosen': \"What do you think is the connection between paper planes and early studies in science?\\nThat's a good start! Flight is definitely related to paper planes. Can you think of someone who might be interested in studying how things fly?\\nExactly! Now, can you think of someone who lived a long time ago, before airplanes were invented, but might still be interested in how things fly?\\nThat's correct! Leonardo da Vinci made many drawings of flying machines, including birds in flight. His studies on bird flight and wing structure actually laid the foundation for modern aerodynamics. And it all started with simple observations, much like throwing a paper plane.\\nSo, to summarize, simple observations like paper planes can lead to early studies in science, just like how Leonardo da Vinci's observations on bird flight inspired his research on aerodynamics. Great job thinking it through!<|im_end|>\", 'prompt': '<|im_start|>system\\nYou are a helpful, harmless, and honest assistant.<|im_end|>\\n<|im_start|>user\\nWhat early studies were inspired by simple observations like paper planes?<|im_end|>\\n<|im_start|>assistant\\n'}\n\nLoading Qwen2.5 model. This may take a few minutes...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d590d98f9884e78bfc5af6dfa13c522"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fc42517b70f47d98494f1bf7ca3293a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f016a5904754bd8937a0ad4663db882"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f564cd5627e543e2bf69c67b433212c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78a42b03d3544c7899486135f67a02a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0925207fa7fa4c678a56583be7f61abd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e14ef5c9735c454094ae34d923ae90ba"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38508a9583204a0fafbece0db500f34b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cf0197443884a30bace8f3529a2c3b4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nInitializing DPO trainer for Qwen2.5...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting prompt in train dataset:   0%|          | 0/660 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a5c4beb16de43e88228f15114953d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Applying chat template to train dataset:   0%|          | 0/660 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48fdc0f9d7dc48eb8443cfe01bd56cd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing train dataset:   0%|          | 0/660 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b97407dca5514c779b47dc0b3e8b9dfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting prompt in eval dataset:   0%|          | 0/74 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69899c2880764eecb10ee42ebe350951"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Applying chat template to eval dataset:   0%|          | 0/74 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba399e444ec54a6eaa19d36142055e4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing eval dataset:   0%|          | 0/74 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff470c082a3a4292be8daf9414d65edc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nStarting DPO training for Qwen2.5...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.19.6"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250505_173907-5d0p3el7</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/mudit-jain2303-mait/huggingface/runs/5d0p3el7' target=\"_blank\">Socratic-Qwen2.5-7B-v2</a></strong> to <a href='https://wandb.ai/mudit-jain2303-mait/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/mudit-jain2303-mait/huggingface' target=\"_blank\">https://wandb.ai/mudit-jain2303-mait/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/mudit-jain2303-mait/huggingface/runs/5d0p3el7' target=\"_blank\">https://wandb.ai/mudit-jain2303-mait/huggingface/runs/5d0p3el7</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 5:35:00, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/chosen</th>\n      <th>Logps/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Logits/rejected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.003500</td>\n      <td>0.001412</td>\n      <td>5.996588</td>\n      <td>-1.601741</td>\n      <td>1.000000</td>\n      <td>7.598329</td>\n      <td>-369.520020</td>\n      <td>-224.416672</td>\n      <td>0.312360</td>\n      <td>-0.676932</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.000200</td>\n      <td>0.000261</td>\n      <td>7.650548</td>\n      <td>-2.222262</td>\n      <td>1.000000</td>\n      <td>9.872809</td>\n      <td>-361.250214</td>\n      <td>-227.519272</td>\n      <td>0.345647</td>\n      <td>-0.645607</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.000200</td>\n      <td>0.000222</td>\n      <td>7.803051</td>\n      <td>-2.286029</td>\n      <td>1.000000</td>\n      <td>10.089080</td>\n      <td>-360.487671</td>\n      <td>-227.838135</td>\n      <td>0.347860</td>\n      <td>-0.644340</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000200</td>\n      <td>0.000216</td>\n      <td>7.824438</td>\n      <td>-2.303201</td>\n      <td>1.000000</td>\n      <td>10.127640</td>\n      <td>-360.380737</td>\n      <td>-227.923981</td>\n      <td>0.348026</td>\n      <td>-0.644479</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nSaving model...\n\nReloading Qwen2.5 base model...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "553f23158ccc49e78d58324d394e9d03"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nMerging with LoRA weights...\n\nSaving merged model...\n\nPushing model to Hugging Face Hub...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc962617a67c40dd9644ef9f378e7e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edb96158125c4197bcfad4728d89a351"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21ecf5006dc24cbb9ad2e4b4ea21513b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cebd94db95840aabc83febad3c25cc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efba37c573614ec3ba51becfd0004fc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "611bfc64219b40c9928736c3202ace84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fb20c5397ea451081c71cff1f978bda"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Ct37uNHK3qHf"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}